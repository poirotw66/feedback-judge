#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æŒ‰ç·¨è™Ÿè©³ç´°æ¯”è¼ƒæº–ç¢ºåº¦çš„å¢å¼·è©•ä¼°å™¨
Enhanced evaluator for detailed accuracy comparison by record ID
"""

import pandas as pd
import numpy as np
from accuracy_evaluator import DisabilityDataEvaluator
from smart_processor import smart_read_excel
import os
from dataclasses import dataclass
from typing import Dict, List, Tuple


@dataclass
class RecordComparison:
    """å–®ç­†è¨˜éŒ„çš„æ¯”è¼ƒçµæœ"""
    record_id: str
    subject_id: str  # å—ç·¨
    field_comparisons: Dict[str, Dict]
    overall_accuracy: float
    total_fields: int
    matched_fields: int


class DetailedEvaluator:
    """è©³ç´°è©•ä¼°å™¨ - æŒ‰ç·¨è™Ÿæ¯”è¼ƒæ¯å€‹æ¬„ä½"""
    
    def __init__(self):
        self.base_evaluator = DisabilityDataEvaluator()
        
    def compare_single_record(self, record_data: Dict[str, Tuple[str, str]], record_id: str, subject_id: str) -> RecordComparison:
        """æ¯”è¼ƒå–®ç­†è¨˜éŒ„çš„æ‰€æœ‰æ¬„ä½"""
        field_comparisons = {}
        total_score = 0.0
        matched_count = 0
        total_fields = 0
        
        for field_name, (correct_value, predicted_value) in record_data.items():
            # è¨ˆç®—ç›¸ä¼¼åº¦
            similarity = self.base_evaluator.calculate_similarity(correct_value, predicted_value)
            is_exact_match = similarity >= 0.99
            
            if is_exact_match:
                matched_count += 1
            
            field_comparisons[field_name] = {
                'æ­£ç¢ºå€¼': str(correct_value) if correct_value is not None else '',
                'é æ¸¬å€¼': str(predicted_value) if predicted_value is not None else '',
                'ç›¸ä¼¼åº¦': similarity,
                'å®Œå…¨åŒ¹é…': is_exact_match,
                'ç‹€æ…‹': 'âœ…' if is_exact_match else 'âŒ' if similarity < 0.5 else 'âš ï¸'
            }
            
            total_score += similarity
            total_fields += 1
        
        overall_accuracy = total_score / total_fields if total_fields > 0 else 0.0
        
        return RecordComparison(
            record_id=record_id,
            subject_id=subject_id,
            field_comparisons=field_comparisons,
            overall_accuracy=overall_accuracy,
            total_fields=total_fields,
            matched_fields=matched_count
        )
    
    def evaluate_all_records(self, file_path: str) -> List[RecordComparison]:
        """è©•ä¼°æ‰€æœ‰è¨˜éŒ„"""
        print(f"æ­£åœ¨é€²è¡ŒæŒ‰ç·¨è™Ÿçš„è©³ç´°æº–ç¢ºåº¦åˆ†æ...")
        
        # è®€å–è³‡æ–™
        df, header_row = smart_read_excel(file_path)
        
        if df is None:
            print("ç„¡æ³•è®€å–æª”æ¡ˆ")
            return []
        
        print(f"æˆåŠŸè®€å– {len(df)} ç­†è¨˜éŒ„")
        
        # è­˜åˆ¥æ¬„ä½
        key_columns = self._identify_columns(df)
        
        if not key_columns:
            print("ç„¡æ³•è­˜åˆ¥å¿…è¦çš„æ¬„ä½")
            return []
        
        print(f"è­˜åˆ¥åˆ°çš„æ¬„ä½é…å°:")
        for field_name, (æ­£é¢_col, åé¢_col) in key_columns.items():
            print(f"  {field_name}: {æ­£é¢_col} vs {åé¢_col}")
        
        # é€ç­†è¨˜éŒ„é€²è¡Œæ¯”è¼ƒ
        record_comparisons = []
        
        for index, row in df.iterrows():
            # å–å¾—ç·¨è™Ÿå’Œå—ç·¨
            record_id = str(row.get('ç·¨è™Ÿ', index + 1))
            subject_id = str(row.get('å—ç·¨', f'è¨˜éŒ„{index + 1}'))
            
            # æº–å‚™æœ¬ç­†è¨˜éŒ„çš„æ¬„ä½è³‡æ–™
            record_data = {}
            
            for field_name, (æ­£é¢_col, åé¢_col) in key_columns.items():
                correct_value = row.get(æ­£é¢_col)
                predicted_value = row.get(åé¢_col)
                record_data[field_name] = (correct_value, predicted_value)
            
            # æ¯”è¼ƒæœ¬ç­†è¨˜éŒ„
            comparison = self.compare_single_record(record_data, record_id, subject_id)
            record_comparisons.append(comparison)
        
        return record_comparisons
    
    def _identify_columns(self, df: pd.DataFrame) -> Dict[str, Tuple[str, str]]:
        """è­˜åˆ¥æ¬„ä½é…å°"""
        key_columns = {}
        cols = list(df.columns)
        
        # éšœç¤™ç­‰ç´š
        for i, col in enumerate(cols):
            if 'éšœç¤™ç­‰ç´š' in str(col) and not str(col).endswith('.1'):
                # å°‹æ‰¾å°æ‡‰çš„åé¢æ¬„ä½
                for j, other_col in enumerate(cols[i+1:], i+1):
                    if 'éšœç¤™ç­‰ç´š' in str(other_col) and j != i:
                        key_columns['éšœç¤™ç­‰ç´š'] = (col, other_col)
                        break
                break
        
        # éšœç¤™é¡åˆ¥
        for i, col in enumerate(cols):
            if 'éšœç¤™é¡åˆ¥' in str(col) and not str(col).endswith('.1') and not str(col).endswith('.2'):
                # å°‹æ‰¾å°æ‡‰çš„åé¢æ¬„ä½
                for j, other_col in enumerate(cols[i+1:], i+1):
                    if 'éšœç¤™é¡åˆ¥' in str(other_col) and j != i and (str(other_col).endswith('.1') or str(other_col).endswith('.2')):
                        key_columns['éšœç¤™é¡åˆ¥'] = (col, other_col)
                        break
                break
        
        # ICDè¨ºæ–·
        for i, col in enumerate(cols):
            if 'ICD' in str(col) and not str(col).endswith('.1') and not str(col).endswith('.2'):
                # å°‹æ‰¾å°æ‡‰çš„åé¢æ¬„ä½
                for j, other_col in enumerate(cols[i+1:], i+1):
                    if 'ICD' in str(other_col) and j != i and (str(other_col).endswith('.1') or str(other_col).endswith('.2')):
                        key_columns['ICDè¨ºæ–·'] = (col, other_col)
                        break
                break
        
        return key_columns
    
    def generate_detailed_report(self, record_comparisons: List[RecordComparison]) -> str:
        """ç”Ÿæˆè©³ç´°å ±å‘Š"""
        if not record_comparisons:
            return "ç„¡æ¯”è¼ƒçµæœ"
        
        report = []
        report.append("=" * 100)
        report.append("æŒ‰ç·¨è™Ÿè©³ç´°æº–ç¢ºåº¦åˆ†æå ±å‘Š")
        report.append("=" * 100)
        
        # æ•´é«”çµ±è¨ˆ
        total_records = len(record_comparisons)
        avg_accuracy = np.mean([comp.overall_accuracy for comp in record_comparisons])
        total_perfect_records = sum(1 for comp in record_comparisons if comp.matched_fields == comp.total_fields)
        
        report.append(f"ğŸ“Š æ•´é«”çµ±è¨ˆ:")
        report.append(f"  ç¸½è¨˜éŒ„æ•¸: {total_records}")
        report.append(f"  å¹³å‡æº–ç¢ºåº¦: {avg_accuracy:.2%}")
        report.append(f"  å®Œå…¨æ­£ç¢ºè¨˜éŒ„: {total_perfect_records}/{total_records} ({total_perfect_records/total_records:.1%})")
        report.append("")
        
        # å„æ¬„ä½çµ±è¨ˆ
        if record_comparisons:
            field_names = list(record_comparisons[0].field_comparisons.keys())
            report.append(f"ğŸ“ˆ å„æ¬„ä½çµ±è¨ˆ:")
            
            for field_name in field_names:
                field_accuracies = []
                field_matches = 0
                
                for comp in record_comparisons:
                    if field_name in comp.field_comparisons:
                        field_accuracies.append(comp.field_comparisons[field_name]['ç›¸ä¼¼åº¦'])
                        if comp.field_comparisons[field_name]['å®Œå…¨åŒ¹é…']:
                            field_matches += 1
                
                avg_field_accuracy = np.mean(field_accuracies) if field_accuracies else 0
                match_rate = field_matches / len(field_accuracies) if field_accuracies else 0
                
                report.append(f"  {field_name}: {avg_field_accuracy:.2%} (å®Œå…¨åŒ¹é…: {field_matches}/{len(field_accuracies)}, {match_rate:.1%})")
            
            report.append("")
        
        # è©³ç´°è¨˜éŒ„åˆ†æ
        report.append(f"ğŸ“‹ è©³ç´°è¨˜éŒ„åˆ†æ:")
        report.append("-" * 100)
        
        for i, comp in enumerate(record_comparisons, 1):
            report.append(f"ã€è¨˜éŒ„ {comp.record_id}ã€‘å—ç·¨: {comp.subject_id}")
            report.append(f"  æ•´é«”æº–ç¢ºåº¦: {comp.overall_accuracy:.2%} ({comp.matched_fields}/{comp.total_fields} å®Œå…¨åŒ¹é…)")
            
            for field_name, field_data in comp.field_comparisons.items():
                status = field_data['ç‹€æ…‹']
                similarity = field_data['ç›¸ä¼¼åº¦']
                correct = field_data['æ­£ç¢ºå€¼']
                predicted = field_data['é æ¸¬å€¼']
                
                report.append(f"    {status} {field_name}: {similarity:.1%}")
                if similarity < 0.99:
                    report.append(f"      æ­£ç¢º: '{correct}'")
                    report.append(f"      é æ¸¬: '{predicted}'")
            
            report.append("")
            
            # æ¯10ç­†è¨˜éŒ„é¡¯ç¤ºä¸€æ¬¡åˆ†éš”ç·š
            if i % 10 == 0 and i < len(record_comparisons):
                report.append("-" * 50)
        
        return "\n".join(report)
    
    def save_detailed_excel(self, record_comparisons: List[RecordComparison], output_path: str):
        """å„²å­˜è©³ç´°Excelçµæœ"""
        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
            # æ•´é«”æ‘˜è¦é 
            summary_data = []
            for comp in record_comparisons:
                summary_data.append({
                    'ç·¨è™Ÿ': comp.record_id,
                    'å—ç·¨': comp.subject_id,
                    'æ•´é«”æº–ç¢ºåº¦': f"{comp.overall_accuracy:.2%}",
                    'å®Œå…¨åŒ¹é…æ¬„ä½æ•¸': comp.matched_fields,
                    'ç¸½æ¬„ä½æ•¸': comp.total_fields,
                    'åŒ¹é…ç‡': f"{comp.matched_fields/comp.total_fields:.1%}" if comp.total_fields > 0 else "0%"
                })
            
            summary_df = pd.DataFrame(summary_data)
            summary_df.to_excel(writer, sheet_name='æ•´é«”æ‘˜è¦', index=False)
            
            # è©³ç´°æ¯”è¼ƒé 
            detailed_data = []
            for comp in record_comparisons:
                for field_name, field_data in comp.field_comparisons.items():
                    detailed_data.append({
                        'ç·¨è™Ÿ': comp.record_id,
                        'å—ç·¨': comp.subject_id,
                        'æ¬„ä½': field_name,
                        'æ­£ç¢ºå€¼': field_data['æ­£ç¢ºå€¼'],
                        'é æ¸¬å€¼': field_data['é æ¸¬å€¼'],
                        'ç›¸ä¼¼åº¦': f"{field_data['ç›¸ä¼¼åº¦']:.2%}",
                        'å®Œå…¨åŒ¹é…': 'æ˜¯' if field_data['å®Œå…¨åŒ¹é…'] else 'å¦',
                        'ç‹€æ…‹': field_data['ç‹€æ…‹']
                    })
            
            detailed_df = pd.DataFrame(detailed_data)
            detailed_df.to_excel(writer, sheet_name='è©³ç´°æ¯”è¼ƒ', index=False)
            
            # å„æ¬„ä½çµ±è¨ˆé 
            if record_comparisons:
                field_names = list(record_comparisons[0].field_comparisons.keys())
                field_stats_data = []
                
                for field_name in field_names:
                    accuracies = []
                    matches = 0
                    total = 0
                    
                    for comp in record_comparisons:
                        if field_name in comp.field_comparisons:
                            accuracies.append(comp.field_comparisons[field_name]['ç›¸ä¼¼åº¦'])
                            if comp.field_comparisons[field_name]['å®Œå…¨åŒ¹é…']:
                                matches += 1
                            total += 1
                    
                    avg_accuracy = np.mean(accuracies) if accuracies else 0
                    match_rate = matches / total if total > 0 else 0
                    
                    field_stats_data.append({
                        'æ¬„ä½åç¨±': field_name,
                        'å¹³å‡æº–ç¢ºåº¦': f"{avg_accuracy:.2%}",
                        'å®Œå…¨åŒ¹é…æ•¸': matches,
                        'ç¸½è¨˜éŒ„æ•¸': total,
                        'åŒ¹é…ç‡': f"{match_rate:.1%}"
                    })
                
                field_stats_df = pd.DataFrame(field_stats_data)
                field_stats_df.to_excel(writer, sheet_name='æ¬„ä½çµ±è¨ˆ', index=False)


def main():
    """ä¸»ç¨‹å¼"""
    evaluator = DetailedEvaluator()
    target_file = "èº«å¿ƒéšœç¤™æ‰‹å†Š_AIæ¸¬è©¦çµæœè³‡æ–™ (1).xlsx"
    
    if not os.path.exists(target_file):
        print(f"æ‰¾ä¸åˆ°æª”æ¡ˆ: {target_file}")
        return
    
    # åŸ·è¡Œè©³ç´°è©•ä¼°
    record_comparisons = evaluator.evaluate_all_records(target_file)
    
    if record_comparisons:
        # ç”Ÿæˆå ±å‘Š
        report = evaluator.generate_detailed_report(record_comparisons)
        print(report)
        
        # å„²å­˜è©³ç´°çµæœ
        output_file = "æŒ‰ç·¨è™Ÿè©³ç´°æº–ç¢ºåº¦åˆ†æ.xlsx"
        evaluator.save_detailed_excel(record_comparisons, output_file)
        print(f"\nè©³ç´°åˆ†æçµæœå·²å„²å­˜è‡³: {output_file}")
    else:
        print("ç„¡æ³•å®Œæˆè©•ä¼°")


if __name__ == "__main__":
    main()
