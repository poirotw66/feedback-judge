#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
èº«å¿ƒéšœç¤™æ‰‹å†ŠAIæ¸¬è©¦çµæœæº–ç¢ºåº¦è©•åˆ†ç³»çµ±
Accuracy Evaluation System for Disability Certificate AI Test Results
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import re
import difflib
from dataclasses import dataclass
from enum import Enum

class FieldType(Enum):
    """æ¬„ä½é¡å‹æšèˆ‰"""
    DISABILITY_LEVEL = "éšœç¤™ç­‰ç´š"
    DISABILITY_CATEGORY = "éšœç¤™é¡åˆ¥"
    ICD_DIAGNOSIS = "ICDè¨ºæ–·"
    CERTIFICATE_TYPE = "è­‰æ˜/æ‰‹å†Š"

@dataclass
class EvaluationResult:
    """è©•ä¼°çµæœè³‡æ–™é¡åˆ¥"""
    field_name: str
    accuracy: float
    exact_matches: int
    total_records: int
    similarity_scores: List[float]
    mismatched_pairs: List[Tuple[str, str]]

@dataclass
class RecordFieldResult:
    """å–®ç­†è¨˜éŒ„çš„æ¬„ä½è©•ä¼°çµæœ"""
    record_id: str
    subject_id: str
    field_name: str
    correct_value: str
    predicted_value: str
    similarity: float
    is_exact_match: bool

@dataclass
class RecordEvaluation:
    """å–®ç­†è¨˜éŒ„çš„å®Œæ•´è©•ä¼°çµæœ"""
    record_id: str
    subject_id: str
    field_results: Dict[str, RecordFieldResult]
    overall_accuracy: float
    total_fields: int
    matched_fields: int

class DisabilityDataEvaluator:
    """èº«å¿ƒéšœç¤™è³‡æ–™æº–ç¢ºåº¦è©•ä¼°å™¨"""
    
    def __init__(self):
        self.similarity_threshold = 0.8
        self.weight_config = {
            FieldType.DISABILITY_LEVEL: 0.3,
            FieldType.DISABILITY_CATEGORY: 0.3,
            FieldType.ICD_DIAGNOSIS: 0.25,
            FieldType.CERTIFICATE_TYPE: 0.15
        }
    
    def normalize_text(self, text: str) -> str:
        """æ¨™æº–åŒ–æ–‡å­—è™•ç†"""
        if pd.isna(text) or text is None:
            return ""
        
        text = str(text).strip()
        # ç§»é™¤å¤šé¤˜çš„ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
        text = re.sub(r'\s+', '', text)
        # çµ±ä¸€æ‹¬è™Ÿæ ¼å¼
        text = text.replace('ã€', '[').replace('ã€‘', ']')
        text = text.replace('ï¼ˆ', '(').replace('ï¼‰', ')')
        return text.lower()
    
    def calculate_similarity(self, text1: str, text2: str) -> float:
        """è¨ˆç®—å…©å€‹æ–‡å­—çš„ç›¸ä¼¼åº¦"""
        norm_text1 = self.normalize_text(text1)
        norm_text2 = self.normalize_text(text2)
        
        if not norm_text1 and not norm_text2:
            return 1.0
        if not norm_text1 or not norm_text2:
            return 0.0
        
        # ä½¿ç”¨SequenceMatcherè¨ˆç®—ç›¸ä¼¼åº¦
        similarity = difflib.SequenceMatcher(None, norm_text1, norm_text2).ratio()
        return similarity
    
    def evaluate_field(self, correct_values: List[str], 
                      predicted_values: List[str], 
                      field_name: str) -> EvaluationResult:
        """è©•ä¼°å–®ä¸€æ¬„ä½çš„æº–ç¢ºåº¦"""
        if len(correct_values) != len(predicted_values):
            raise ValueError(f"æ­£ç¢ºå€¼å’Œé æ¸¬å€¼çš„æ•¸é‡ä¸ä¸€è‡´: {len(correct_values)} vs {len(predicted_values)}")
        
        exact_matches = 0
        similarity_scores = []
        mismatched_pairs = []
        
        for correct, predicted in zip(correct_values, predicted_values):
            similarity = self.calculate_similarity(correct, predicted)
            similarity_scores.append(similarity)
            
            if similarity >= 0.99:  # è¿‘ä¼¼å®Œå…¨åŒ¹é…
                exact_matches += 1
            elif similarity < self.similarity_threshold:
                mismatched_pairs.append((str(correct), str(predicted)))
        
        accuracy = np.mean(similarity_scores)
        
        return EvaluationResult(
            field_name=field_name,
            accuracy=accuracy,
            exact_matches=exact_matches,
            total_records=len(correct_values),
            similarity_scores=similarity_scores,
            mismatched_pairs=mismatched_pairs
        )
    
    def load_data_from_excel(self, file_path: str) -> pd.DataFrame:
        """å¾Excelæª”æ¡ˆè¼‰å…¥è³‡æ–™"""
        try:
            df = pd.read_excel(file_path)
            return df
        except Exception as e:
            print(f"è¼‰å…¥Excelæª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return None
    
    def create_sample_data(self) -> pd.DataFrame:
        """å»ºç«‹ç¯„ä¾‹è³‡æ–™ï¼ˆåŸºæ–¼æ‚¨æä¾›çš„è³‡æ–™æ ¼å¼ï¼‰"""
        data = {
            'ç·¨è™Ÿ': [1, 2],
            'å—ç·¨': ['ZA24761194', 'MT00953431'],
            'æ­£é¢_éšœç¤™ç­‰ç´š': ['è¼•åº¦', 'ä¸­åº¦'],
            'æ­£é¢_éšœç¤™é¡åˆ¥': ['å…¶ä»–é¡', 'ç¬¬1é¡ã€12.2ã€‘'],
            'æ­£é¢_ICDè¨ºæ–·': ['ã€æ›16.1ã€‘', 'ã€æ›12.2ã€‘'],
            'æ­£é¢_å‚™è¨»': ['', ''],
            'åé¢_éšœç¤™ç­‰ç´š': ['è¼•åº¦', 'ä¸­åº¦'],
            'åé¢_è­‰æ˜æ‰‹å†Š': ['èº«å¿ƒéšœç¤™è­‰æ˜', 'èº«å¿ƒéšœç¤™è­‰æ˜'],
            'åé¢_éšœç¤™é¡åˆ¥': ['éšœç¤™é¡åˆ¥ï¼šå…¶ä»–é¡', 'ç¬¬1é¡ã€12.2ã€‘'],
            'åé¢_ICDè¨ºæ–·': ['ã€æ›16.1ã€‘', 'ã€ç¬¬12.2ã€‘']
        }
        return pd.DataFrame(data)
    
    def evaluate_all_fields(self, df: pd.DataFrame) -> Dict[str, EvaluationResult]:
        """è©•ä¼°æ‰€æœ‰æ¬„ä½çš„æº–ç¢ºåº¦"""
        results = {}
        
        # å®šç¾©æ¬„ä½å°æ‡‰é—œä¿‚ï¼ˆæ­£é¢ vs åé¢ï¼‰
        field_mappings = {
            'éšœç¤™ç­‰ç´š': ('æ­£é¢_éšœç¤™ç­‰ç´š', 'åé¢_éšœç¤™ç­‰ç´š'),
            'éšœç¤™é¡åˆ¥': ('æ­£é¢_éšœç¤™é¡åˆ¥', 'åé¢_éšœç¤™é¡åˆ¥'),
            'ICDè¨ºæ–·': ('æ­£é¢_ICDè¨ºæ–·', 'åé¢_ICDè¨ºæ–·')
        }
        
        for field_name, (correct_col, predicted_col) in field_mappings.items():
            if correct_col in df.columns and predicted_col in df.columns:
                correct_values = df[correct_col].tolist()
                predicted_values = df[predicted_col].tolist()
                
                result = self.evaluate_field(correct_values, predicted_values, field_name)
                results[field_name] = result
            else:
                print(f"è­¦å‘Š: æ‰¾ä¸åˆ°æ¬„ä½ {correct_col} æˆ– {predicted_col}")
        
        return results
    
    def calculate_overall_accuracy(self, results: Dict[str, EvaluationResult]) -> float:
        """è¨ˆç®—æ•´é«”åŠ æ¬Šæº–ç¢ºåº¦"""
        total_weighted_accuracy = 0.0
        total_weight = 0.0
        
        for field_name, result in results.items():
            # æ ¹æ“šæ¬„ä½é¡å‹å–å¾—æ¬Šé‡
            weight = 0.25  # é è¨­æ¬Šé‡
            for field_type in FieldType:
                if field_type.value in field_name:
                    weight = self.weight_config.get(field_type, 0.25)
                    break
            
            total_weighted_accuracy += result.accuracy * weight
            total_weight += weight
        
        return total_weighted_accuracy / total_weight if total_weight > 0 else 0.0
    
    def evaluate_record_fields(self, record_data: Dict[str, Tuple[str, str]], 
                              record_id: str, subject_id: str = None) -> RecordEvaluation:
        """è©•ä¼°å–®ç­†è¨˜éŒ„ä¸­æ¯å€‹æ¬„ä½çš„æº–ç¢ºåº¦"""
        field_results = {}
        total_score = 0.0
        matched_count = 0
        
        for field_name, (correct_value, predicted_value) in record_data.items():
            # è¨ˆç®—ç›¸ä¼¼åº¦
            similarity = self.calculate_similarity(correct_value, predicted_value)
            is_exact_match = similarity >= 0.99
            
            if is_exact_match:
                matched_count += 1
            
            # å»ºç«‹æ¬„ä½çµæœ
            field_result = RecordFieldResult(
                record_id=record_id,
                subject_id=subject_id or record_id,
                field_name=field_name,
                correct_value=str(correct_value) if correct_value is not None else '',
                predicted_value=str(predicted_value) if predicted_value is not None else '',
                similarity=similarity,
                is_exact_match=is_exact_match
            )
            
            field_results[field_name] = field_result
            total_score += similarity
        
        # è¨ˆç®—æ•´é«”æº–ç¢ºåº¦
        overall_accuracy = total_score / len(record_data) if record_data else 0.0
        
        return RecordEvaluation(
            record_id=record_id,
            subject_id=subject_id or record_id,
            field_results=field_results,
            overall_accuracy=overall_accuracy,
            total_fields=len(record_data),
            matched_fields=matched_count
        )
    
    def evaluate_all_records(self, df: pd.DataFrame, 
                           field_mappings: Dict[str, Tuple[str, str]] = None) -> List[RecordEvaluation]:
        """è©•ä¼°æ‰€æœ‰è¨˜éŒ„ä¸­æ¯å€‹æ¬„ä½çš„æº–ç¢ºåº¦"""
        if field_mappings is None:
            # é è¨­æ¬„ä½å°æ‡‰é—œä¿‚
            field_mappings = {
                'éšœç¤™ç­‰ç´š': ('æ­£é¢_éšœç¤™ç­‰ç´š', 'åé¢_éšœç¤™ç­‰ç´š'),
                'éšœç¤™é¡åˆ¥': ('æ­£é¢_éšœç¤™é¡åˆ¥', 'åé¢_éšœç¤™é¡åˆ¥'),
                'ICDè¨ºæ–·': ('æ­£é¢_ICDè¨ºæ–·', 'åé¢_ICDè¨ºæ–·')
            }
        
        record_evaluations = []
        
        for index, row in df.iterrows():
            # å–å¾—ç·¨è™Ÿå’Œå—ç·¨
            record_id = str(row.get('ç·¨è™Ÿ', index + 1))
            subject_id = str(row.get('å—ç·¨', f'è¨˜éŒ„{index + 1}'))
            
            # æº–å‚™æœ¬ç­†è¨˜éŒ„çš„æ¬„ä½è³‡æ–™
            record_data = {}
            
            for field_name, (correct_col, predicted_col) in field_mappings.items():
                if correct_col in df.columns and predicted_col in df.columns:
                    correct_value = row.get(correct_col)
                    predicted_value = row.get(predicted_col)
                    record_data[field_name] = (correct_value, predicted_value)
            
            if record_data:
                # è©•ä¼°æœ¬ç­†è¨˜éŒ„
                evaluation = self.evaluate_record_fields(record_data, record_id, subject_id)
                record_evaluations.append(evaluation)
        
        return record_evaluations
    
    def generate_record_report(self, record_evaluations: List[RecordEvaluation]) -> str:
        """ç”ŸæˆæŒ‰è¨˜éŒ„çš„è©³ç´°è©•ä¼°å ±å‘Š"""
        if not record_evaluations:
            return "ç„¡è©•ä¼°çµæœ"
        
        report = []
        report.append("=" * 80)
        report.append("èº«å¿ƒéšœç¤™æ‰‹å†ŠAIæ¸¬è©¦çµæœ - æŒ‰ç·¨è™Ÿæ¬„ä½æº–ç¢ºåº¦è©•ä¼°å ±å‘Š")
        report.append("=" * 80)
        
        # æ•´é«”çµ±è¨ˆ
        total_records = len(record_evaluations)
        avg_accuracy = np.mean([eval_result.overall_accuracy for eval_result in record_evaluations])
        perfect_records = sum(1 for eval_result in record_evaluations if eval_result.matched_fields == eval_result.total_fields)
        
        report.append(f"ğŸ“Š æ•´é«”çµ±è¨ˆ:")
        report.append(f"  ç¸½è¨˜éŒ„æ•¸: {total_records}")
        report.append(f"  å¹³å‡æº–ç¢ºåº¦: {avg_accuracy:.2%}")
        report.append(f"  å®Œå…¨æ­£ç¢ºè¨˜éŒ„: {perfect_records}/{total_records} ({perfect_records/total_records:.1%})")
        report.append("")
        
        # å„æ¬„ä½çµ±è¨ˆ
        if record_evaluations:
            field_names = list(record_evaluations[0].field_results.keys())
            report.append(f"ğŸ“ˆ å„æ¬„ä½çµ±è¨ˆ:")
            
            for field_name in field_names:
                field_accuracies = []
                field_matches = 0
                
                for evaluation in record_evaluations:
                    if field_name in evaluation.field_results:
                        field_result = evaluation.field_results[field_name]
                        field_accuracies.append(field_result.similarity)
                        if field_result.is_exact_match:
                            field_matches += 1
                
                avg_field_accuracy = np.mean(field_accuracies) if field_accuracies else 0
                match_rate = field_matches / len(field_accuracies) if field_accuracies else 0
                
                report.append(f"  {field_name}: {avg_field_accuracy:.2%} (å®Œå…¨åŒ¹é…: {field_matches}/{len(field_accuracies)}, {match_rate:.1%})")
            
            report.append("")
        
        # è©³ç´°è¨˜éŒ„åˆ†æ
        report.append(f"ğŸ“‹ è©³ç´°è¨˜éŒ„åˆ†æ:")
        report.append("-" * 80)
        
        for i, evaluation in enumerate(record_evaluations, 1):
            report.append(f"ã€è¨˜éŒ„ {evaluation.record_id}ã€‘å—ç·¨: {evaluation.subject_id}")
            report.append(f"  æ•´é«”æº–ç¢ºåº¦: {evaluation.overall_accuracy:.2%} ({evaluation.matched_fields}/{evaluation.total_fields} å®Œå…¨åŒ¹é…)")
            
            for field_name, field_result in evaluation.field_results.items():
                status = "âœ…" if field_result.is_exact_match else "âŒ" if field_result.similarity < 0.5 else "âš ï¸"
                report.append(f"    {status} {field_name}: {field_result.similarity:.1%}")
                
                if not field_result.is_exact_match:
                    report.append(f"      æ­£ç¢º: '{field_result.correct_value}'")
                    report.append(f"      é æ¸¬: '{field_result.predicted_value}'")
            
            report.append("")
        
        return "\n".join(report)
    
    def save_record_results(self, record_evaluations: List[RecordEvaluation], 
                           output_path: str = "record_accuracy_details.xlsx"):
        """å„²å­˜æŒ‰è¨˜éŒ„çš„è©³ç´°çµæœåˆ°Excelæª”æ¡ˆ"""
        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
            # è¨˜éŒ„æ‘˜è¦é 
            summary_data = []
            for evaluation in record_evaluations:
                summary_data.append({
                    'ç·¨è™Ÿ': evaluation.record_id,
                    'å—ç·¨': evaluation.subject_id,
                    'æ•´é«”æº–ç¢ºåº¦': f"{evaluation.overall_accuracy:.2%}",
                    'å®Œå…¨åŒ¹é…æ¬„ä½æ•¸': evaluation.matched_fields,
                    'ç¸½æ¬„ä½æ•¸': evaluation.total_fields,
                    'åŒ¹é…ç‡': f"{evaluation.matched_fields/evaluation.total_fields:.1%}" if evaluation.total_fields > 0 else "0%",
                    'ç‹€æ…‹æè¿°': f"({evaluation.matched_fields}/{evaluation.total_fields} å®Œå…¨åŒ¹é…)"
                })
            
            summary_df = pd.DataFrame(summary_data)
            summary_df.to_excel(writer, sheet_name='è¨˜éŒ„æ‘˜è¦', index=False)
            
            # è©³ç´°æ¬„ä½æ¯”è¼ƒé  - å¢å¼·ç‰ˆ
            detailed_data = []
            for evaluation in record_evaluations:
                for field_name, field_result in evaluation.field_results.items():
                    status_icon = 'âœ…' if field_result.is_exact_match else 'âŒ' if field_result.similarity < 0.5 else 'âš ï¸'
                    status_text = 'å®Œå…¨åŒ¹é…' if field_result.is_exact_match else 'ä¸åŒ¹é…' if field_result.similarity < 0.5 else 'éƒ¨åˆ†åŒ¹é…'
                    
                    detailed_data.append({
                        'ç·¨è™Ÿ': field_result.record_id,
                        'å—ç·¨': field_result.subject_id,
                        'æ¬„ä½': field_name,
                        'ç‹€æ…‹åœ–ç¤º': status_icon,
                        'ç‹€æ…‹': status_text,
                        'ç›¸ä¼¼åº¦': f"{field_result.similarity:.1%}",
                        'ç›¸ä¼¼åº¦æ•¸å€¼': field_result.similarity,
                        'æ­£ç¢ºå€¼': field_result.correct_value,
                        'é æ¸¬å€¼': field_result.predicted_value,
                        'å®Œå…¨åŒ¹é…': 'æ˜¯' if field_result.is_exact_match else 'å¦',
                        'å·®ç•°æè¿°': 'å®Œå…¨ç›¸åŒ' if field_result.is_exact_match else f"ç›¸ä¼¼åº¦: {field_result.similarity:.1%}"
                    })
            
            detailed_df = pd.DataFrame(detailed_data)
            detailed_df.to_excel(writer, sheet_name='è©³ç´°æ¬„ä½æ¯”è¼ƒ', index=False)
            
            # è¨˜éŒ„è©³æƒ…é  - æ–°å¢ï¼Œæ ¼å¼åŒ–é¡¯ç¤ºæ¯ç­†è¨˜éŒ„çš„å®Œæ•´è³‡è¨Š
            record_details_data = []
            for evaluation in record_evaluations:
                # ç‚ºæ¯ç­†è¨˜éŒ„æ·»åŠ æ¨™é¡Œè¡Œ
                record_details_data.append({
                    'è¨˜éŒ„ç·¨è™Ÿ': f"ã€è¨˜éŒ„ {evaluation.record_id}ã€‘",
                    'å—ç·¨': evaluation.subject_id,
                    'æ•´é«”æº–ç¢ºåº¦': f"{evaluation.overall_accuracy:.2%}",
                    'åŒ¹é…æƒ…æ³': f"({evaluation.matched_fields}/{evaluation.total_fields} å®Œå…¨åŒ¹é…)",
                    'æ¬„ä½åç¨±': '',
                    'ç‹€æ…‹': '',
                    'ç›¸ä¼¼åº¦': '',
                    'æ­£ç¢ºå€¼': '',
                    'é æ¸¬å€¼': '',
                    'å‚™è¨»': '=== è¨˜éŒ„æ¨™é¡Œ ==='
                })
                
                # æ·»åŠ æ¯å€‹æ¬„ä½çš„è©³ç´°è³‡è¨Š
                for field_name, field_result in evaluation.field_results.items():
                    status_icon = 'âœ…' if field_result.is_exact_match else 'âŒ' if field_result.similarity < 0.5 else 'âš ï¸'
                    
                    record_details_data.append({
                        'è¨˜éŒ„ç·¨è™Ÿ': '',
                        'å—ç·¨': '',
                        'æ•´é«”æº–ç¢ºåº¦': '',
                        'åŒ¹é…æƒ…æ³': '',
                        'æ¬„ä½åç¨±': field_name,
                        'ç‹€æ…‹': f"{status_icon} {field_result.similarity:.1%}",
                        'ç›¸ä¼¼åº¦': f"{field_result.similarity:.1%}",
                        'æ­£ç¢ºå€¼': field_result.correct_value,
                        'é æ¸¬å€¼': field_result.predicted_value,
                        'å‚™è¨»': 'å®Œå…¨åŒ¹é…' if field_result.is_exact_match else 'éœ€è¦æ”¹é€²' if field_result.similarity < 0.5 else 'éƒ¨åˆ†åŒ¹é…'
                    })
                
                # æ·»åŠ ç©ºè¡Œåˆ†éš”
                record_details_data.append({
                    'è¨˜éŒ„ç·¨è™Ÿ': '',
                    'å—ç·¨': '',
                    'æ•´é«”æº–ç¢ºåº¦': '',
                    'åŒ¹é…æƒ…æ³': '',
                    'æ¬„ä½åç¨±': '',
                    'ç‹€æ…‹': '',
                    'ç›¸ä¼¼åº¦': '',
                    'æ­£ç¢ºå€¼': '',
                    'é æ¸¬å€¼': '',
                    'å‚™è¨»': '--- åˆ†éš”ç·š ---'
                })
            
            record_details_df = pd.DataFrame(record_details_data)
            record_details_df.to_excel(writer, sheet_name='è¨˜éŒ„è©³æƒ…', index=False)
            
            # å„æ¬„ä½çµ±è¨ˆé 
            if record_evaluations:
                field_names = list(record_evaluations[0].field_results.keys())
                field_stats_data = []
                
                for field_name in field_names:
                    accuracies = []
                    matches = 0
                    total = 0
                    
                    for evaluation in record_evaluations:
                        if field_name in evaluation.field_results:
                            field_result = evaluation.field_results[field_name]
                            accuracies.append(field_result.similarity)
                            if field_result.is_exact_match:
                                matches += 1
                            total += 1
                    
                    avg_accuracy = np.mean(accuracies) if accuracies else 0
                    match_rate = matches / total if total > 0 else 0
                    
                    field_stats_data.append({
                        'æ¬„ä½åç¨±': field_name,
                        'å¹³å‡æº–ç¢ºåº¦': f"{avg_accuracy:.2%}",
                        'å®Œå…¨åŒ¹é…æ•¸': matches,
                        'ç¸½è¨˜éŒ„æ•¸': total,
                        'åŒ¹é…ç‡': f"{match_rate:.1%}",
                        'éœ€æ”¹é€²è¨˜éŒ„æ•¸': total - matches,
                        'è¡¨ç¾ç­‰ç´š': 'å„ªç§€' if avg_accuracy >= 0.9 else 'è‰¯å¥½' if avg_accuracy >= 0.7 else 'éœ€æ”¹é€²'
                    })
                
                field_stats_df = pd.DataFrame(field_stats_data)
                field_stats_df.to_excel(writer, sheet_name='æ¬„ä½çµ±è¨ˆ', index=False)
            
            # éŒ¯èª¤åˆ†æé  - æ–°å¢ï¼Œå°ˆé–€åˆ†æéœ€è¦æ”¹é€²çš„é …ç›®
            error_analysis_data = []
            for evaluation in record_evaluations:
                for field_name, field_result in evaluation.field_results.items():
                    if not field_result.is_exact_match:
                        error_type = 'æ ¼å¼å·®ç•°' if field_result.similarity > 0.7 else 'å…§å®¹éŒ¯èª¤' if field_result.similarity > 0.3 else 'å®Œå…¨ä¸ç¬¦'
                        
                        error_analysis_data.append({
                            'ç·¨è™Ÿ': field_result.record_id,
                            'å—ç·¨': field_result.subject_id,
                            'æ¬„ä½': field_name,
                            'éŒ¯èª¤é¡å‹': error_type,
                            'ç›¸ä¼¼åº¦': f"{field_result.similarity:.1%}",
                            'æ­£ç¢ºå€¼': field_result.correct_value,
                            'é æ¸¬å€¼': field_result.predicted_value,
                            'æ”¹é€²å»ºè­°': self._get_improvement_suggestion(field_result)
                        })
            
            if error_analysis_data:
                error_analysis_df = pd.DataFrame(error_analysis_data)
                error_analysis_df.to_excel(writer, sheet_name='éŒ¯èª¤åˆ†æ', index=False)
    
    def _get_improvement_suggestion(self, field_result: RecordFieldResult) -> str:
        """ç‚ºæ¬„ä½éŒ¯èª¤æä¾›æ”¹é€²å»ºè­°"""
        if field_result.similarity > 0.8:
            return "æ ¼å¼æ¨™æº–åŒ– - ç›¸ä¼¼åº¦é«˜ï¼Œä¸»è¦æ˜¯æ ¼å¼å•é¡Œ"
        elif field_result.similarity > 0.5:
            return "å…§å®¹æª¢æŸ¥ - éƒ¨åˆ†æ­£ç¢ºï¼Œéœ€è¦ç´°ç¯€èª¿æ•´"
        elif field_result.similarity > 0.2:
            return "é‡æ–°è¨“ç·´ - è­˜åˆ¥éŒ¯èª¤ï¼Œéœ€è¦åŠ å¼·ç›¸é—œè³‡æ–™è¨“ç·´"
        else:
            return "å®Œå…¨é‡åš - è­˜åˆ¥å¤±æ•—ï¼Œéœ€è¦é‡æ–°è™•ç†æˆ–æ‰‹å‹•æª¢æŸ¥"
    
    def generate_report(self, results: Dict[str, EvaluationResult], 
                       overall_accuracy: float) -> str:
        """ç”Ÿæˆè©•ä¼°å ±å‘Š"""
        report = []
        report.append("=" * 60)
        report.append("èº«å¿ƒéšœç¤™æ‰‹å†ŠAIæ¸¬è©¦çµæœæº–ç¢ºåº¦è©•ä¼°å ±å‘Š")
        report.append("=" * 60)
        report.append(f"æ•´é«”æº–ç¢ºåº¦: {overall_accuracy:.2%}")
        report.append("")
        
        for field_name, result in results.items():
            report.append(f"ã€{field_name}ã€‘")
            report.append(f"  æº–ç¢ºåº¦: {result.accuracy:.2%}")
            report.append(f"  å®Œå…¨åŒ¹é…: {result.exact_matches}/{result.total_records} "
                         f"({result.exact_matches/result.total_records:.1%})")
            
            if result.mismatched_pairs:
                report.append(f"  ä¸åŒ¹é…é …ç›®æ•¸: {len(result.mismatched_pairs)}")
                report.append("  ä¸»è¦ä¸åŒ¹é…é …ç›®:")
                for i, (correct, predicted) in enumerate(result.mismatched_pairs[:3]):
                    report.append(f"    {i+1}. æ­£ç¢º: '{correct}' -> é æ¸¬: '{predicted}'")
                if len(result.mismatched_pairs) > 3:
                    report.append(f"    ... é‚„æœ‰ {len(result.mismatched_pairs)-3} é …")
            report.append("")
        
        return "\n".join(report)
    
    def save_detailed_results(self, results: Dict[str, EvaluationResult], 
                             output_path: str = "accuracy_details.xlsx"):
        """å„²å­˜è©³ç´°çµæœåˆ°Excelæª”æ¡ˆ"""
        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
            # æ‘˜è¦é 
            summary_data = []
            for field_name, result in results.items():
                summary_data.append({
                    'æ¬„ä½åç¨±': field_name,
                    'æº–ç¢ºåº¦': f"{result.accuracy:.2%}",
                    'å®Œå…¨åŒ¹é…æ•¸': result.exact_matches,
                    'ç¸½è¨˜éŒ„æ•¸': result.total_records,
                    'åŒ¹é…ç‡': f"{result.exact_matches/result.total_records:.1%}",
                    'ä¸åŒ¹é…é …ç›®æ•¸': len(result.mismatched_pairs)
                })
            
            summary_df = pd.DataFrame(summary_data)
            summary_df.to_excel(writer, sheet_name='æ‘˜è¦', index=False)
            
            # è©³ç´°éŒ¯èª¤é 
            for field_name, result in results.items():
                if result.mismatched_pairs:
                    error_data = []
                    for correct, predicted in result.mismatched_pairs:
                        similarity = self.calculate_similarity(correct, predicted)
                        error_data.append({
                            'æ­£ç¢ºå€¼': correct,
                            'é æ¸¬å€¼': predicted,
                            'ç›¸ä¼¼åº¦': f"{similarity:.2%}"
                        })
                    
                    error_df = pd.DataFrame(error_data)
                    sheet_name = f"{field_name}_éŒ¯èª¤è©³æƒ…"[:31]  # Excelå·¥ä½œè¡¨åç¨±é™åˆ¶
                    error_df.to_excel(writer, sheet_name=sheet_name, index=False)

def main():
    """ä¸»ç¨‹å¼"""
    evaluator = DisabilityDataEvaluator()
    
    # ä½¿ç”¨ç¯„ä¾‹è³‡æ–™é€²è¡Œç¤ºç¯„
    print("ä½¿ç”¨ç¯„ä¾‹è³‡æ–™é€²è¡Œæº–ç¢ºåº¦è©•ä¼°...")
    df = evaluator.create_sample_data()
    
    # å¦‚æœæœ‰å¯¦éš›çš„Excelæª”æ¡ˆï¼Œå¯ä»¥å–æ¶ˆè¨»è§£ä»¥ä¸‹è¡Œ
    # df = evaluator.load_data_from_excel("èº«å¿ƒéšœç¤™æ‰‹å†Š_AIæ¸¬è©¦çµæœè³‡æ–™.xlsx")
    
    if df is not None:
        print(f"è¼‰å…¥è³‡æ–™æˆåŠŸï¼Œå…± {len(df)} ç­†è¨˜éŒ„")
        
        # å‚³çµ±æ•´é«”è©•ä¼°
        print("\n=== å‚³çµ±æ•´é«”è©•ä¼° ===")
        results = evaluator.evaluate_all_fields(df)
        overall_accuracy = evaluator.calculate_overall_accuracy(results)
        report = evaluator.generate_report(results, overall_accuracy)
        print(report)
        
        # æ–°çš„æŒ‰è¨˜éŒ„è©•ä¼°
        print("\n=== æŒ‰ç·¨è™Ÿå€‹åˆ¥æ¬„ä½è©•ä¼° ===")
        record_evaluations = evaluator.evaluate_all_records(df)
        record_report = evaluator.generate_record_report(record_evaluations)
        print(record_report)
        
        # å„²å­˜çµæœ
        evaluator.save_detailed_results(results, "æ•´é«”æº–ç¢ºåº¦è©•ä¼°.xlsx")
        evaluator.save_record_results(record_evaluations, "æŒ‰ç·¨è™Ÿæ¬„ä½æº–ç¢ºåº¦è©•ä¼°.xlsx")
        print("\nè©³ç´°çµæœå·²å„²å­˜è‡³:")
        print("- æ•´é«”æº–ç¢ºåº¦è©•ä¼°.xlsx")
        print("- æŒ‰ç·¨è™Ÿæ¬„ä½æº–ç¢ºåº¦è©•ä¼°.xlsx")
        
    else:
        print("ç„¡æ³•è¼‰å…¥è³‡æ–™ï¼Œè«‹æª¢æŸ¥æª”æ¡ˆè·¯å¾‘")

if __name__ == "__main__":
    main()
